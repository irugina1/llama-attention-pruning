## Attention logits outputs (for first example in test set):
    transformers implementation on a100:
        tensor([[[ 0.1006, -0.2217,  0.3145,  ...,  1.3262,  1.8779,  0.6411],
                 [-2.3652,  7.4141,  9.1172,  ...,  4.3438, -0.2732,  3.2949],
                 [-7.0625, -2.7637,  4.9961,  ..., -4.7227, -4.6016, -1.7617],
                 ...,
                 [-3.0527, -4.5352,  6.3672,  ..., -1.6670,  1.3428,  0.3564],
                 [-2.7441, -8.0391,  3.6602,  ..., -0.9058, -2.4629,  0.4844],
                 [-5.6133, -9.6797,  2.7539,  ..., -2.0859, -0.0535, -1.8867]]],
               device='cuda:0', dtype=torch.float16)


    transformers implementation on local mps
        tensor([[[ 0.1028, -0.2238,  0.3149,  ...,  1.3262,  1.8789,  0.6426],
                 [-2.3809,  7.4062,  9.1094,  ...,  4.3359, -0.2812,  3.2910],
                 [-7.0625, -2.7422,  4.9688,  ..., -4.7031, -4.5820, -1.7490],
                 ...,
                 [-3.0527, -4.5430,  6.3633,  ..., -1.6650,  1.3486,  0.3496],
                 [-2.7480, -8.0234,  3.6758,  ..., -0.9077, -2.4727,  0.4812],
                 [-5.6172, -9.6875,  2.7500,  ..., -2.0938, -0.0581, -1.8975]]],
               device='mps:0', dtype=torch.float16)

    transformers implementation on a100 with deepspeed
       tensor([[[ 0.1006, -0.2217,  0.3145,  ...,  1.3262,  1.8779,  0.6411],
                [-2.3789,  7.3984,  9.1094,  ...,  4.3398, -0.2800,  3.2930],
                [-7.0703, -2.7715,  4.9883,  ..., -4.7266, -4.6055, -1.7686],
                ...,
                [-3.0527, -4.5352,  6.3672,  ..., -1.6650,  1.3477,  0.3545],
                [-2.7480, -8.0547,  3.6504,  ..., -0.9092, -2.4688,  0.4875],
                [-5.6172, -9.6875,  2.7480,  ..., -2.0898, -0.0546, -1.8945]]],
       device='cuda:0', dtype=torch.float16)

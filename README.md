# llama-attention-pruning

Llama2 inference with data-informed sparse attention on modal labs A100 using deepspeed.

```
@misc{rugina2021datainformed,
      title={Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks}, 
      author={Ileana Rugina and Rumen Dangovski and Li Jing and Preslav Nakov and Marin Soljačić},
      year={2021},
      eprint={2012.02030},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

# llama-attention-pruning

Llama2 inference with data-informed sparse attention on modal labs A100 using deepspeed.

```
@misc{hasan2024pruning,
      title={Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning}, 
      author={Adib Hasan and Ileana Rugina and Alex Wang},
      year={2024},
      eprint={2401.10862},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```
